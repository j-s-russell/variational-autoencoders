{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Variational Autoencoders\n",
        "\n",
        "The autoencoder is an extremely useful type of neural network due to its ability to learn important lower-dimensional representations of data. Its bottleneck architecture has been modified to create several other interesting types of networks for various problems, such as sparse autoencoders and denoising autoencoders. One of the most interesting variations is the variational autoencoder - an architecture designed for generative modeling. As autoencoders map their inputs onto a lower-dimensional latent space, Diederik Kingma and Max Welling had the idea to use this latent space and the decoder network to generate new samples. A traditional autoencoder maps its input onto a fixed vector of real numbers, so sampling from this latent space is unlikely to produce any meaningful content as this space is not regularized for generative processes. The defining feature of a variational autoencoder is that its input is mapped onto a distribution. When this latent distribution is properly regularized, the decoder network can take samples from it to produce novel examples. In this project, these examples will be the handwritten digits from the MNIST dataset."
      ],
      "metadata": {
        "id": "o1NInj6_bWJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Packages and Preprocessing Data"
      ],
      "metadata": {
        "id": "8pb-Jg1SPmR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "VALkd_njQOtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_val, y_val) = tf.keras.datasets.mnist.load_data(path='mnist.npz')\n",
        "x_train, x_val = x_train / 255.0, x_val / 255.0"
      ],
      "metadata": {
        "id": "qpNEJxGRSoVN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "734f7db2-24e7-478c-f34b-b12bfb28e142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.reshape(x_train, [-1, 28, 28, 1])\n",
        "x_val = np.reshape(x_val, (-1, 28, 28, 1))\n",
        "print(x_train.shape, x_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNFjkPh9Uaxn",
        "outputId": "76c25a53-0541-4938-a46a-e7af098f9939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28, 1) (10000, 28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General Architecture\n",
        "\n",
        "Given an input $x$ and its latent representation $z$, the VAE is trained to map $x$ onto the latent distribution $P(z|x)$, from which a sample is taken and used to reconstruct the input by sampling from the distribution $P(x|z)$."
      ],
      "metadata": {
        "id": "m7uaTrIuWQ-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a VAE, the bottleneck vector is replaced by two vectors - a mean vector and standard deviation vector representing the parameters of *P(z|x)*. The encoder takes an input *x*, maps it to these two vectors, and then outputs a sample from this latent distribution *z ~ P(z|x)*, which is then fed into the decoder. Given this sampled latent variable *z*, the decoder is trained to map *z* to back to *x*. Thus the encoder and decoder can be defined as *P(z|x)* and *P(x|z)* respectively."
      ],
      "metadata": {
        "id": "Yg9Wg6-cdzv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling\n",
        "\n",
        "Below is a function that samples from the latent distribution $P(z|x)$ to be used in generating the encoder's output. It's important to mention that if this network were to randomly from the mean and standard deviation vectors of the latent distribution, it would be impossible to perform backpropagation through this random layer. This function instead uses a reparameterization trick:"
      ],
      "metadata": {
        "id": "HmzCeg_cQNxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$z = \\mu + \\epsilon(e^{\\sigma/2})$$"
      ],
      "metadata": {
        "id": "TKUFGd25ftXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "where $\\mu$ and $\\sigma$ represent the mean and standard deviation of the latent variable $z$, and $\\epsilon$ represents random noise from a standard normal distribution. This way, a random sample of $z$ is produced, but the only stochastic variable is $\\epsilon$, so backpropagation can be ran through $\\mu$ and $\\sigma$."
      ],
      "metadata": {
        "id": "g5NM0KNGft10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def sample(mu, sigma):\n",
        "    epsilon = K.random_normal(shape=(K.shape(mu)))\n",
        "    return mu + K.exp(0.5 * sigma) * epsilon"
      ],
      "metadata": {
        "id": "9gLJEi7DQVex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder\n",
        " "
      ],
      "metadata": {
        "id": "haKwBwRiQXdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 12\n",
        "\n",
        "encoder_input = tf.keras.layers.Input(shape=(28, 28, 1),)\n",
        "\n",
        "hidden = tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu', padding='same', strides=2)(encoder_input)\n",
        "encoder_output = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same', strides=2)(hidden)\n",
        "shape = K.int_shape(encoder_output)\n",
        "\n",
        "hidden2 = tf.keras.layers.Flatten()(encoder_output)\n",
        "hidden2 = tf.keras.layers.Dense(16, activation='relu')(hidden2)\n",
        "z_mean = tf.keras.layers.Dense(latent_dim, name='z_mean')(hidden2)\n",
        "z_log_var = tf.keras.layers.Dense(latent_dim, name='z_log_var')(hidden2)\n",
        "z = sample(z_mean, z_log_var)\n",
        "\n",
        "encoder = tf.keras.Model(encoder_input, [z_mean, z_log_var, z], name='encoder')\n",
        "encoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGN-MsMOQX09",
        "outputId": "f85655e1-5cfc-4b1a-b786-3d1a36910a96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 14, 14, 16)   160         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 7, 7, 32)     4640        ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 1568)         0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 16)           25104       ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " z_mean (Dense)                 (None, 12)           204         ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " z_log_var (Dense)              (None, 12)           204         ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " tf.math.multiply (TFOpLambda)  (None, 12)           0           ['z_log_var[0][0]']              \n",
            "                                                                                                  \n",
            " tf.compat.v1.shape (TFOpLambda  (2,)                0           ['z_mean[0][0]']                 \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.exp (TFOpLambda)       (None, 12)           0           ['tf.math.multiply[0][0]']       \n",
            "                                                                                                  \n",
            " tf.random.normal (TFOpLambda)  (None, 12)           0           ['tf.compat.v1.shape[0][0]']     \n",
            "                                                                                                  \n",
            " tf.math.multiply_1 (TFOpLambda  (None, 12)          0           ['tf.math.exp[0][0]',            \n",
            " )                                                                'tf.random.normal[0][0]']       \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 12)          0           ['z_mean[0][0]',                 \n",
            " da)                                                              'tf.math.multiply_1[0][0]']     \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 30,312\n",
            "Trainable params: 30,312\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "ROBOuA1fQYKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input = tf.keras.layers.Input(shape=(latent_dim,), name='z_sampling')\n",
        "hidden = tf.keras.layers.Dense(shape[1] * shape[2] * shape[3], activation='relu')(decoder_input)\n",
        "hidden = tf.keras.layers.Reshape((shape[1], shape[2], shape[3]))(hidden)\n",
        "\n",
        "hidden = tf.keras.layers.Conv2DTranspose(filters=16, kernel_size=3,activation='relu', padding='same', strides=2)(hidden)\n",
        "hidden = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3,activation='relu', padding='same', strides=2)(hidden)\n",
        "decoder_output = tf.keras.layers.Conv2DTranspose(1, kernel_size=3, activation='relu', padding='same')(hidden)\n",
        "\n",
        "decoder = tf.keras.Model(decoder_input, decoder_output, name='decoder')\n",
        "decoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8og6PRGQYim",
        "outputId": "6b13d5ab-7bb3-4caa-9ff2-4ab63861382c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " z_sampling (InputLayer)     [(None, 12)]              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1568)              20384     \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 7, 7, 32)          0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 14, 14, 16)       4624      \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 28, 28, 32)       4640      \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 28, 28, 1)        289       \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 29,937\n",
            "Trainable params: 29,937\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construct the Variational Autoencoder\n",
        "\n",
        "After defining the encoder and decoder, we can construct the VAE by simply combining the two models:"
      ],
      "metadata": {
        "id": "F2jFTdEgQY5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vae_output = decoder(encoder(encoder_input)[2])\n",
        "vae = tf.keras.Model(encoder_input, vae_output, name='vae')"
      ],
      "metadata": {
        "id": "iHirkwvbQ0oT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deriving the Loss Function - Variational Inference\n",
        "\n",
        "We can assume that $P(z)$ follows a standard normal distribution and that $P(x|z)$ follows a normal distribution. Since we're sampling $x$ from $z$, we can describe the mean of $P(x|z)$ as a function of $z$:"
      ],
      "metadata": {
        "id": "IoDqF2Lps3tg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ P(z) \\sim \\mathcal{N}(0, I) $$"
      ],
      "metadata": {
        "id": "ePs5u8FIw8Gi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$P(x|z) \\sim \\mathcal{N}(f(z),cI)$$"
      ],
      "metadata": {
        "id": "TXKQC0LCwgxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$I$ in the above definitions represent the identity matrix, used to describe the distributions' covariances. The only remaining distribution of interest is the distribution given by the encoder, $P(z|x)$. However, $P(z|x)$ is often an intractable computation, so instead the VAE has to approximate it somehow. VAEs get their name from variational inference, the technique used to approximate this unknown distribution. The technique involves defining a family of distributions used to approximate this distribution. Once defined, the approximation is made by minimizing an error function that makes the approximated distribution as similar as possible to the target distribution."
      ],
      "metadata": {
        "id": "HRUsDFUbyhE1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, $P(z|x)$ is approximated by a normal distribution $Q_x(z)$, whose mean and covariance are represented by the functions $g(x)$ and $h(x)$:"
      ],
      "metadata": {
        "id": "DtSz1mkm_u9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ Q_x(z) \\sim \\mathcal{N}(g(x),h(x)) $$"
      ],
      "metadata": {
        "id": "Ma5nSVxNBSgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the objective is to minimize the a measure of distance between $Q_x(z)$ and $P(z|x)$. In most cases of variational inference, this measure is the Kullback-Leibler Divergence, which is defined as:"
      ],
      "metadata": {
        "id": "Rycf9b4TCMGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ D_{KL}(Q||P) = \\int P(x)*\\log \\left (\\frac{Q(x)}{P(x)} \\right ) dx$$"
      ],
      "metadata": {
        "id": "mu8sapzmGyHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given $Q_x(z)$ and $P(z|x)$, we have the following derivation:"
      ],
      "metadata": {
        "id": "M3nmM2FSLSEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ D_{KL}\\left (Q_x(z) || P(z|x) \\right ) = \\int Q_x(z)*\\log \\frac{Q_x(z)}{P(z|x)} dz$$\n",
        "\n",
        "$$ = \\int Q_x(z)*\\log \\frac{Q_x(z)P(x)}{P(z,x)} dz $$\n",
        "\n",
        "$$ = \\int Q_x(z)* \\left (\\log P(x) + \\log (\\frac{Q_x(z)}{P(z,x)} \\right ) dz $$\n",
        "\n",
        "$$ = \\log P(x) + \\int Q_x(z)*\\log \\frac{Q_x(z)}{P(z,x)}dz $$\n",
        "\n",
        "$$ = \\log P(x)+ \\int Q_x(z)*\\log \\frac{Q_x(z)}{P(x|z)P(z))} dz $$\n",
        "\n",
        "$$ = \\log P(x) - E_{z\\sim Q_x(z)}\\left (\\log P(x|z)\\right ) + \\int Q_x(z)*\\log \\frac{Q_x(z)}{P(z))} $$\n",
        "\n",
        "$$ = \\log P(x) - E_{z\\sim Q_x(z)}\\left (\\log P(x|z)\\right ) + D_{KL}\\left ( Q_x(z)||P(z)\\right )$$"
      ],
      "metadata": {
        "id": "6ViV6CoLLVwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we defined $P(x|z) \\sim \\mathcal{N}(f(z),cI)$, we can replace $\\log P(x|z)$ with $-\\frac{||x-f(z)||^2}{2c}$. We're trying to minimize this function with respect to $f(z)$, the mean of $P(x|z)$, and $g(x)$ and $h(x)$, the parameters of $Q_x(z)$. $\\log P(x)$ is fixed, so we can ignore that term - our function to minimize is then"
      ],
      "metadata": {
        "id": "r4jF_bRttBPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ D_{KL}\\left (Q_x(z) || P(z|x) \\right ) = E_{z\\sim Q_x(z)}\\left ( \\frac{||x-f(z)||^2}{2c}\\right ) + D_{KL}\\left ( Q_x(z)||P(z)\\right )$$"
      ],
      "metadata": {
        "id": "lJ8Qn7skVJ3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interestingly, we can identify the first term on the right hand side as a form of the reconstruction cost. This term is minimized when the mean of $P(x|z)$, $f(z)$ is equal to $x$ - in other words, the decoder recreates the inputs $x$ most accurately when the average output of its sampling process is equivalent to the input itself. The second term is the KL Divergence between our approximation of our encoder's distribution, $P(z|x)$ and the standard normal latent space distribution $P(z)$. This can be thought of as the regularization term of the function which ensures that the distribution of our encoder's output is similar to the standard normal. This is important because when minimizing only the reconstruction cost, the VAE can overfit and learn like a traditional autoencoder - variances can be very small and means can vary significantly. Due to these issues sampling from the latent space distribution is not guaranteed to produce a meaningful output. Regularizing the encoder's distribution to be close to the standard normal ensures that means are all centered around 0 and standard deviations close to 1. This ensures a relatively smooth, complete, and continuous latent space where points are not too separated and nearby points have similar outputs."
      ],
      "metadata": {
        "id": "kJvR7b2vVefU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Loss Function\n",
        "\n",
        "In practice, the reconstruction cost of a VAE is expressed using either mean squared error or binary cross-entropy. In this case, we'll use binary cross-entropy:"
      ],
      "metadata": {
        "id": "U-2CE279Jlzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reconstruction_cost = tf.keras.losses.binary_crossentropy(K.flatten(encoder_input), K.flatten(vae_output))\n",
        "reconstruction_cost *= 784"
      ],
      "metadata": {
        "id": "EXeReYZrJnQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the regularization term, there is a closed-form of the Kullback-Leibler Divergence that is a function of only our distribution's mean and variance:"
      ],
      "metadata": {
        "id": "imyTXMc1eFXU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$D_{KL}\\left (Q_x(z) || \\mathcal{N}(0,1) \\right ) = \\frac{1}{2}\\sum \\left (1 + \\log(\\sigma^2) - \\mu^2 - \\sigma^2 \\right)$$"
      ],
      "metadata": {
        "id": "9yr5TTzRej7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "kl_loss = K.sum(kl_loss, axis=-1)\n",
        "kl_loss *= -0.5"
      ],
      "metadata": {
        "id": "bNlCMa5MeFsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After defining the loss function, we add it to our model and compile the VAE:\n"
      ],
      "metadata": {
        "id": "-TehD4FYJpFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = K.mean(reconstruction_cost + kl_loss)\n",
        "\n",
        "vae.add_loss(loss)\n",
        "vae.compile(optimizer='adam')\n",
        "vae.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnlCzXXVJs7v",
        "outputId": "15a95c0e-7802-4fc7-a7a7-b7da884452a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vae\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " encoder (Functional)           [(None, 12),         30312       ['input_1[0][0]']                \n",
            "                                 (None, 12),                                                      \n",
            "                                 (None, 12)]                                                      \n",
            "                                                                                                  \n",
            " decoder (Functional)           (None, 28, 28, 1)    29937       ['encoder[0][2]']                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 14, 14, 16)   160         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 7, 7, 32)     4640        ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 1568)         0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 16)           25104       ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " z_log_var (Dense)              (None, 12)           204         ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " z_mean (Dense)                 (None, 12)           204         ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " tf.reshape (TFOpLambda)        (None,)              0           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " tf.reshape_1 (TFOpLambda)      (None,)              0           ['decoder[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None, 12)          0           ['z_log_var[0][0]']              \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.math.square (TFOpLambda)    (None, 12)           0           ['z_mean[0][0]']                 \n",
            "                                                                                                  \n",
            " tf.cast (TFOpLambda)           (None,)              0           ['tf.reshape[0][0]']             \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor (TFOpLamb  (None,)             0           ['tf.reshape_1[0][0]']           \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf.math.subtract (TFOpLambda)  (None, 12)           0           ['tf.__operators__.add_1[0][0]', \n",
            "                                                                  'tf.math.square[0][0]']         \n",
            "                                                                                                  \n",
            " tf.math.exp_1 (TFOpLambda)     (None, 12)           0           ['z_log_var[0][0]']              \n",
            "                                                                                                  \n",
            " tf.keras.backend.binary_crosse  (None,)             0           ['tf.cast[0][0]',                \n",
            " ntropy (TFOpLambda)                                              'tf.convert_to_tensor[0][0]']   \n",
            "                                                                                                  \n",
            " tf.math.subtract_1 (TFOpLambda  (None, 12)          0           ['tf.math.subtract[0][0]',       \n",
            " )                                                                'tf.math.exp_1[0][0]']          \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean (TFOpLambd  ()                  0           ['tf.keras.backend.binary_crossen\n",
            " a)                                                              tropy[0][0]']                    \n",
            "                                                                                                  \n",
            " tf.math.reduce_sum (TFOpLambda  (None,)             0           ['tf.math.subtract_1[0][0]']     \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.multiply_2 (TFOpLambda  ()                  0           ['tf.math.reduce_mean[0][0]']    \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.multiply_3 (TFOpLambda  (None,)             0           ['tf.math.reduce_sum[0][0]']     \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TFOpLa  (None,)             0           ['tf.math.multiply_2[0][0]',     \n",
            " mbda)                                                            'tf.math.multiply_3[0][0]']     \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_1 (TFOpLam  ()                  0           ['tf.__operators__.add_2[0][0]'] \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " add_loss (AddLoss)             ()                   0           ['tf.math.reduce_mean_1[0][0]']  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 60,249\n",
            "Trainable params: 60,249\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "0HLdSWoyQ1y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vae.fit(x_train, epochs = 15, batch_size = 128, shuffle = True, validation_data = (x_val,None))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUsk5sQHQ2Fl",
        "outputId": "d787edab-6321-428e-ff91-da13c232bdc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "469/469 [==============================] - 62s 129ms/step - loss: 204.1629 - val_loss: 154.8271\n",
            "Epoch 2/15\n",
            "469/469 [==============================] - 61s 131ms/step - loss: 152.0452 - val_loss: 142.1706\n",
            "Epoch 3/15\n",
            "469/469 [==============================] - 59s 127ms/step - loss: 145.3740 - val_loss: 139.0027\n",
            "Epoch 4/15\n",
            "469/469 [==============================] - 60s 127ms/step - loss: 143.1444 - val_loss: 139.6306\n",
            "Epoch 5/15\n",
            "469/469 [==============================] - 59s 126ms/step - loss: 149.2629 - val_loss: 147.1347\n",
            "Epoch 6/15\n",
            "469/469 [==============================] - 60s 127ms/step - loss: 138.9909 - val_loss: 134.4212\n",
            "Epoch 7/15\n",
            "469/469 [==============================] - 59s 126ms/step - loss: 146.2814 - val_loss: 139.5435\n",
            "Epoch 8/15\n",
            "469/469 [==============================] - 59s 125ms/step - loss: 140.5928 - val_loss: 134.8914\n",
            "Epoch 9/15\n",
            "469/469 [==============================] - 59s 125ms/step - loss: 134.2273 - val_loss: 140.6591\n",
            "Epoch 10/15\n",
            "469/469 [==============================] - 59s 125ms/step - loss: 140.5667 - val_loss: 137.1107\n",
            "Epoch 11/15\n",
            "469/469 [==============================] - 58s 125ms/step - loss: 132.2053 - val_loss: 128.2392\n",
            "Epoch 12/15\n",
            "469/469 [==============================] - 60s 127ms/step - loss: 131.7969 - val_loss: 135.2667\n",
            "Epoch 13/15\n",
            "469/469 [==============================] - 61s 130ms/step - loss: 129.4612 - val_loss: 132.1374\n",
            "Epoch 14/15\n",
            "469/469 [==============================] - 61s 130ms/step - loss: 131.9190 - val_loss: 248.0061\n",
            "Epoch 15/15\n",
            "469/469 [==============================] - 61s 130ms/step - loss: 149.2450 - val_loss: 134.1239\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f84f916d4d0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Generation\n",
        "\n",
        "After training the VAE, we can use the decoder to replicate the handwriten digits of the MNIST dataset:"
      ],
      "metadata": {
        "id": "kPkiVmJ_Q9yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z_mean, _, _ = encoder.predict(x_val)\n",
        "decoded_imgs = decoder.predict(z_mean)\n",
        "\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(10):\n",
        "\tplt.gray()\n",
        "\tax = plt.subplot(2, n, i+1)\n",
        "\tplt.imshow(x_val[i].reshape(28, 28))\n",
        "\tax.get_xaxis().set_visible(False)\n",
        "\tax.get_yaxis().set_visible(False)\n",
        "\t\n",
        "\tax = plt.subplot(2, n, i +1+n)\n",
        "\tplt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "\tax.get_xaxis().set_visible(False)\n",
        "\tax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "m_RmS7GHQ-uO",
        "outputId": "f8026d70-2359-4e77-8d68-7596f52c82ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x288 with 20 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "With a 12-dimensional latent space, the generated images are somewhat blurry, but are decently recognizable. Well-designed variational autoencoders can produce highly impressive results across a wide variety of inputs, which serve as an inspiration for the potential of deep generative modeling. Another important and very interesting generative framework is the Generative Adversarial Network introduced by Ian Goodfellow in 2014, which overcomes many of the shortcomings of VAEs and can generate images virtually indistinguishable from real examples. From image generation to data augmentation, this field has some of the most exciting applications in all of machine learning and is bound to produce spectacular findings in the near future."
      ],
      "metadata": {
        "id": "y5MvylzPRDnG"
      }
    }
  ]
}